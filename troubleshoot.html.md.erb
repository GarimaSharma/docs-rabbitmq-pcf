---
title: Troubleshooting
owner: London Services
---

This topic provides basic instructions for operators troubleshooting On-Demand RabbitMQ&reg; for PCF.


## <a id="create-instance"></a> Errors


### <a id="create-instance"></a> Instance Creation Errors

This section describes errors that may occur when a developer runs `cf create-service` to create an instance of the on-demand RabbitMQ for PCF service. To properly debug the error, you need the error message.

<table border='1' class='nice'>
<tr>
  <th width="22%">Error</th>
  <td><p><code>Instance provisioning failed: There was a problem completing your request. Please contact your operations team providing the following information: <br>
  service: RABBITMQ-SERVICE-NAME, service-instance-guid: SERVICE-INSTANCE-GUID, broker-request-id: REQUEST-GUID, task-id: TASK-ID, operation: create</code></p>

  <p>The GUID identifiers are long alphanumeric strings. TASK-ID is a shorter number.</p>
 </td>
</tr>
<tr>
  <th>Possible Causes</th>
  <td>
    <ul>
    <li>BOSH failure</li>
    <li>Service Adapter failure</li>
    </ul>
  </td>
</tr>
<tr>
  <th>Solution</th>
  <td>
    <ol>
      <li>Check the BOSH task information:</li>
        <ol><li>Get BOSH login details</li>
        <li><code>bosh task $task-id</code> from the error above</li>
        <li>Follow BOSH debugging workflow</li></ol>
      <li>If the BOSH error shows a problem with the deployment manifest:
        <ol><li>Access the broker logs (if they have syslog then use the tool they forward to, otherwise <code>bosh logs broker 0</code>, if they have more than 1 instance of the broker, collect logs from each broker)</li>
        <li>Use the broker-request-id from the error message above to search the log for more information.</li>
        <li>Contact Pivotal support with this information.</li></ol>
      </li>
    </ol>
   </td>
</tr>
</table>

<table border='1' class='nice'>
<tr>
  <th width="22%">Error</th>
  <td><p><code>TK ERROR TEXT</code></p>

  <p>TK ONE-LINE ERROR EXPLANATION</p>
 </td>
</tr>
<tr>
  <th>Possible Causes</th>
  <td>
    <ul>
    <li>TK CAUSE 1</li>
    <li>TK CAUSE 2</li>
    </ul>
  </td>
</tr>
<tr>
  <th>Solution</th>
  <td>
    <p>TK SOLUTION</p>

    <ol><li>
      TK STEP 1
    </li><li>
      TK STEP 2
    </li></ol>

    <p>TK OTHER STUFF</p>
  </td>
</tr>
</table>

### <a id="network"></a> Network Failure

The network fails to deliver messages between the Cloud Controller, On-Demand Broker, Service Adapter, and BOSH.


### <a id="stop"></a> <i>bosh stop</i> Failure

If the `bosh stop` fails, you will likely get an error saying that the drain
script failed with:

```
result: 1 of 1 drain scripts failed. Failed Jobs: rabbitmq-server.
```

#### What do I do when bosh stop rabbitmq-server fails?

The drain script logs to `/var/vcap/sys/log/rabbtimq-server/drain.log`. If you
have a remote syslog configured, this will appear as the `rmq_server_drain`
program.

First, `bosh ssh` into the failing rabbitmq-server instance and start the
rabbimtq-server job by running `monit start rabbitmq-server`). You will not be
able to start the job via `bosh start` as this always runs the drain script first
and will fail since the drain script is failing. Once rabbitmq-server job is
running (confirm this via `monit status`), run `DEBUG=1`
`/var/vcap/jobs/rabbitmq-server/bin/drain`. This will tell you exactly why itâ€™s
failing.

### <a id="smoke"></a> Smoke Test Failure

If errors occur while the smoke tests run, they will be summarised at the end of the errand log output.
Detailed logs can be found where the failure occurs.

When encountering an error when running smoke tests, it can be helpful to search the log for other instances
of the error summary printed at the end of the tests, e.g. <code>Failed to target Cloud Foundry</code>.
Lookout for <code>TIP: ...</code> in the logs next to any error output for further troubleshooting hints.

### <a id="ipsec-installation-issues"></a> IPsec Installation Issues

#### Symptom
Unresponsive applications or incomplete responses, particularly for large payloads

#### Explanation: Packet Loss
IPsec packet encryption increases the size of packet payloads on host VMs. If the size of the larger packets exceeds the maximum transmission unit (MTU) size of the host VM, packet loss may occur when the VM forwards those packets.

If your VMs were created with an Amazon PV stemcell, the default MTU value is 1500 for both host VMs and the application containers. If your VMs were created with Amazon HVM stemcells, the default MTU value is 9001. Garden containers default to 1500 MTU.

#### Solution
Implement a 100 MTU difference between host VM and the contained application container, using one of the following approaches:

* Decrease the MTU of the application containers to a value lower than the MTU of the VM for that container. In the ERT tile configuration, click **Networking** and modify **Applications Network Maximum Transmission Unit (MTU) (in bytes)** before you deploy. Decrease it from the default value of 1454 to 1354.

* Increase the MTU of the application container VMs to a value greater than 1500. Pivotal recommends a headroom of 100. Run <code>ifconfig NETWORK-INTERFACE mtu MTU-VALUE</code> to make this change. Replace NETWORK-INTERFACE with the network interface used to communicate with other VMs For example:
<code>$ ifconfig NETWORK-INTERFACE mtu 1600</code>

## <a id="techniques"></a> Techniques

### <a id="determine-task-id"></a> Determine the Task ID of a <i>create-service</i> Task

This procedure determines the Task ID for the BOSH task that creates (or attempts to create) an on-demand service instance, if you only know the the service name. This BOSH task is triggered by the `cf create-service` command.

1. Use the cf CLI to target and to log in to your deployment. See [Login](https://docs.pivotal.io/pivotalcf/cf-cli/getting-started.html#login) the *Getting Started with the cf CLI* topic.
    <pre class="terminal">
    $ cf login -a https<span>://</span>api.example.com -u username<span>@</span>example.com
    API endpoint: https<span>://</span>api.example.com
    </pre>

1. Follow the directions to [SSH into Ops Manager](https://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html#ssh) in the *Advanced Troubleshooting with the BOSH CLI* topic. Use the following values:

    * **Certificate Location**: Use the file path on your local machine to the key pair used to install Ops Manager.
    * **BOSH Director IP Address**: Retrieve from the Ops Manager Director tile **Status** tab.
    * **BOSH Director Credentials**: Retrieve from the Ops Manager API or find it in the Ops Man Director tile **Credentials** tab > **Director Credentials**.

1. Continue following the *Advanced Troubleshooting* directions to [Log into BOSH](https://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html#log-in). 

1. From the Ops Manager Director VM you can now use the BOSH CLI to retrieve information about service instances from the BOSH Director VM.
    <pre class="terminal">
    $ bosh --ca-cert CERTIFICATE-LOCATON target BOSH-DIRECTOR-IP
    </pre>

bosh tasks recent

bosh tasks recent | grep GUID # first column gives you TASK-ID # for instance

bosh task TASK-ID # shows full error message

bosh task TASK-ID --raw # shows raw JSON of error

### <a id="determine-guid"></a> Determine the GUID of an Instance

This procedure determines the GUID of a service instance if you only know the service name. It works whether or not the service instance was created successfully.

1. Log into the space containing the instance or failed instance. 
    <pre class="terminal">
    $ cf login
    </pre>

1. Run `cf services` to see a listing of

cf services - only one line on each

cf service SERVICE-INSTANCE-NAME # more info on specific instance

cf service SERVICE-INSTANCE-NAME --guid # to get GUID of instance

### <a id="set-up-logging"></a> Set Up Logging

This procedure sets up logging for RabbitMQ for PCF, to send log messages from RabbitMQ for PCF service components to a logging platform.

1. If you haven't already, set up an account with a <a href="https://docs.pivotal.io/pivotalcf/devguide/services/log-management-thirdparty-svc.html">logging service</a>.

1. In Ops Manager, click into the **RabbitMQ for PCF** tile.

1. In the **Settings** tab > **Syslog** pane enter the following:
  * For **Syslog address**, enter the address of your logging service endpoint
  * For **Syslog port**, enter the port number of your logging service endpoint

1. Click **Save**.

1. In the **Installation Dashboard**, click **Apply Changes**.

This setup only forwards logs from RabbitMQ for PCF components, not from platform components like the Cloud Controller. To forward platform component logs, open the Elastic Runtime tile **Syslog** pane and configure your logging endpoint address and port number as the **Syslog address** and **Syslog port** fields.


### <a id="check-logs"></a> Check Logs

Once you have the BOSH Task ID and instance GUID, proceed as follows:</p>

1. Run <code>bosh task TASK-ID</code> to show instance creation task errors from BOSH. Possible errors include failure at the IaaS level and hitting your rate limit for the day.

1. If the BOSH task completed successfully, the problem was elsewhere. You can use a logging platform to locate where. If you haven't already, follow the instructions to <a href="#set-up-logging">set up logging</a> to track the progress of your <code>create-service</code>task. Note that RabbitMQ for PCF logs and platform component logs are configured separately.

1. Search for the service instance GUID in the logs generated by <code>cf create-service</code>.

1. Refer to the <a href="https://docs.pivotal.io/svc-sdk/odb/concepts.html#create-service-instance">Create service instance process diagram</a> to see the order of component actions and messages that successfully create an on-demand service instance. The Cloud Controller component generates the service instance GUID before it sends the first <code>provision instance</code> call to the on-demand broker, and all subsequent messages related to the instance creation task include this GUID.

1. Along with the process diagram, refer to these component identifiers in the logs to determine how far the <code>create-service</code> process went and where it failed:
      <ul><li><code>tk_cloud_controller_xyz</code> - Cloud Controller</li>
      <li><code>tk_odb_xyz</code> - On-Demand Broker</li>
      <li><code>tk_jobs_rabbitmq_server</code> - RabbitMQ for PCF Service Adapter</li>
      <li><code>tk_bosh_xyz</code> - BOSH</li></ul>
    </li></ol>

### <a id="stop"></a> Stop RabbitMQ

This procedure safely stops RabbitMQ. You may need to do this when debugging, on your own or working with GSS.

`bosh stop rabbitmq-server`

There are BOSH job lifecycle hooks which are only fired when rabbitmq-server is
stopped through BOSH. This will cleanly stop the RabbitMQ service and is the only supported method of stopping the service.

**DO NOT STOP RABBITMQ VIA MONIT**

Do not log into the virtual machine running the RabbitMQ service and stop the RabbitMQ service via monit. It is a potentially dangerous operation as it will kill the RabbitMQ service immediately.

#### What happens when I run bosh stop rabbitmq-server?

BOSH starts the shutdown sequence from the bootstrap instance.

We start by telling the RabbitMQ application to shutdown and then shutdown the
Erlang VM within which it is running. If this succeeds, we run the following
checks to ensure that the RabbitMQ application and Erlang VM have stopped:

1. If `/var/vcap/sys/run/rabbitmq-server/pid` exists, check that the PID inside
this file does not point to a running Erlang VM process. Notice that we are
tracking the Erlang PID and not the RabbitMQ PID.
1. Check that `rabbitmqctl` does not return an Erlang VM PID

Once this completes on the bootstrap instance, BOSH will continue the same
sequence on the next instance. All remaining rabbitmq-server instances will be
stopped one by one.
